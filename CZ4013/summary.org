#+SETUPFILE: setup.org
#+TITLE: CZ4013 Distributed Systems Summary
#+AUTHOR: Eric Leonardo Lim
* Tasks
** TODO Chandy & Lamport Algorithm at Global States
** TODO Data-Centric Consistency Models & Protocols
** TODO Complete document (2 sections left)
* Introduction
** What is DS?
*** Definition
A set of networked computers that _communicate_ and _coordinate_ their actions only by _passing messages_.
** Fundamental Characteristics of DS
*** Concurrency
- More computer \rightarrow Higher capacity \rightarrow Higher performance
- Need coordination
*** Loosely coupled
- No global clock
  - difficult sync
  - lack certainty
  - difficult algo design
- No global shared memory
  - interaction by message passing
*** Independent failures
- difficult detection
- more fault-tolerant if properly designed
** Main Motivation of DS: Resource Sharing
*** Description
We can share resources (e.g. information, hardware, software) over a DS to save cost and manpower.

Resources are shared via _services_. Server accepts client requests, performs services, responds to clients. Requests and replies are sent in form of messages.
*** Service
Distinct part of computer system providing accesses to managed resources
*** Server
Process managing resources
*** Client
Process requesting resources access 
*** Remote Invocation
a complete interaction between client & server. (request to response).
** DS Issues and Problems
*** Heterogeneity
**** Networks
**** Hardware
**** OS
**** Programming languages
*** Scalability
**** Approaches to handle increasing demand
- Caching and data replication
- Deploy multiple servers
**** Avoid bottlenecks
- Prefer decentralized to centralized
**** Prevent SW resources from running out
- e.g. IP length
*** Failure handling
**** Detecting
e.g. checksum for corruption
**** Masking
e.g. retransmission for losts
**** Tolerating
use redundancy
**** Recovery
- rollback when crashing
- not leaving data in inconsistent state
** Architectural Models
*** SW and HW layers (vertical)
**** Platform
Bring system's programming interface up to a level that facilitates communication and coordination between processes.
***** Hardware
***** OS
**** Middleware
SW layer to mask heterogeneity and provide convenient programming model.
Provide generic services to applications, e.g.:
- Naming
- Security
- Persistent storage
e.g. Sun RPC, Java RMI, CORBA
**** Applications/services
*** System architectures (horizontal)
**** Client-server
Simple yet useful. Division of work among servers: partitioning and replication.
***** Server
Processes that manage resources
***** Client
Processes that use/access service
**** P2P
- All processes play similar roles
- No distinction of roles
- More complex
** Fundamental Models
More formal description of common and intrinsic properties
*** Interaction Model
**** Synchronous DS
- Bounds on
  - Processing time
  - Transmission time
  - Clock drift rate
- Infer properties
  - Timeout to detect failures
  - Bound lag
**** Asynchronous DS
- No bounds
- Model internet
- Can't use timeouts to detect failures
*** Failure Model
**** Omission failure
***** Process
Crash
***** Communication
Fail to deliver
**** Byzantine failure
- cause trouble
- arbitrarily omit intended steps
- arbitrarily take unintended steps
- process
  - set wrong values
  - return wrong result
- communication
  - corruption
  - more than once
* Interprocess Communication
** External Data Representation & Mashalling
*** Marshal & Unmarshal
- data transmitted as sequence of bytes
- data structures must be flattened
- different representations of same datatype
  - big endian: MSB first
  - little endian: LSB first
- extrenal data representation
  - agreed standard for representation
- interprocess communication
  - define standard
  - marshalling
    - structured & primitive \rightarrow external data representation
  - unmarshalling
    - external data representation \rightarrow structured & primitive
*** CORBA's Common Data Representation
- can represent all primitive and constructed datatypes
- types of data items are not given
- it is assumed that sender and recipient have common knowledge of order and types of data items
*** Java object serialization
Deserialization does not require prior knowledge of object types.

#+BEGIN_SRC java
class Person implements Serializable{
  private String name;
  private String place;
  private int year;
}
#+END_SRC

Person {"Smith","London",1934} will be serialized as:
- Person
- Version number (8 bytes)
- h0 (class handle)
- 3 (# of fields)
- int year
- java.lang.String name
- java.lang.String place
- 1934
- 5 Smith
- 6 London
- h1 (object handle)

#+BEGIN_SRC java
class Person implements Serializable{
  private Person one;
  private Person two;
  public Couple(Person a, Person b){
    one = a;
    two = b;
  }
}
#+END_SRC

Couple { {"Smith","London",1934} , {"Jones","Paris",1945} } will be serialized as:
- Couple
- 8 byte version number
- h0
- 2
- Person one
- Person two
- Person
- 8 byte version number
- h1
- 3
- int year
- java.lang.String name
- java.lang.String place
- 1934
- 5 Smith
- 6 London
- h2
- 1945
- 5 Jones
- 5 Paris
- h3
- h4

** Client-Server Communication
*** Reliability
**** Validity
Message reaches destination
**** Integrity
- Message received = message sent
- Received only once
*** UDP
**** Features
- no ack or retries
- fragmentation for message larger than max
- not reliable
  - integrity
    - checksum
  - no validity
    - communication omission failure
- for quick response, can assume reliability, manual fault handling
**** Goal
To build reliable protocol over UDP:
- use timeout
  - solve lost request problem
  - but lost reply may imply more than once execution if operation is non-idempotent
- use requestID to filter out duplicates

*** TCP
**** Features
- abstraction of reliable stream
  - connection-oriented
    - setup connection before data transfer
  - ack and retries
  - tranparent message size segmentation
- reliable
  - integrity
    - checksum
    - sequence number
  - validity
    - ack
    - timeouts
- overhead
  - store state info
  - transmission overhead
  - latency from connection setup
**** Goal
To reduce of overhead of protocols over TCP:
- send requests and replies over same connection
  - avoid multiple TCP slow-starts
  - amortize connection establishment overhad
* Distributed Objects & Remote Invocation
** Object Model
consists of a collection of interacting objects
- communicate by invoking methods
- to invoke a method, need...
    - object reference
    - method name
    - arguments
- interface defines method signatures
    - definitions only
    - no implementation
    - implementation can be changed without changing interface
- exceptions:
    - errors
    - unexpected conditions
** Distributed Object Model
*** Description
- servers manage objects, clients invoke methods w/ RMI
- remote method invocation
  - between objects in different processes
- local method invocation
  - between objects in same process
- remote objects
  - objects that can receive remote invocations
- to invoke a method, need
  - remote object reference
    - created by server
    - obtained by client to access object
  - method name
  - arguments
- remote interface
  - specifies methods that can be invoked remotely
  - defined by
    - Java interface
    - CORBA Interface Definition Language (IDL)
- exception raised by RMI due to
  - distribution
    - timeouts
    - lost messages
  - method exceptions

*** Invocation Semantics
**** Maybe
- No retransmission
- If result, server executed method.
- If no result,
  - communication omission failure
    - request lost
      - not executed
    - reply lost
      - executed
  - process omission failure
    - executed/not executed
- e.g. CORBA for void methods
**** At-least-once
- Retransmission
- If result, server executed method \geq1x
- If no result, ??
- Acceptable if idempotent
- e.g. Sun RPC
**** At-most-once
- Retransmission and duplicate-filtering
- If result, server executed method 1x
- If no result, \leq1x
- e.g. Java RMI and CORBA
** Architecture of RMI
*** Proxy
- 1 proxy foreach class of remote object reference
- proxy implements methods in remote interface
  - transparency of RMI
- marshals arguments and unmarshals results
*** Communication Modules
- transmit request/reply
- e.g. messageType + requestID + remoteObjectReference + methodID + arguments
*** Skeleton
- 1 skeleton foreach class of remote object
- implements methods in remote interface
- unmarshals arguments, invokes method, marshals results
*** Binder
- name service that maintains mappings
  - object name \rightarrow remote object references
- server register remote objects by name in binders
- clients look up references
*** Description
- proxy and skeleton automatically generated by interface compiler
  - Java \rightarrow Java RMI compiler
  - CORBA \rightarrow use interface compiler
- server
  - skeleton
  - real implementation of methods
- client
  - proxy

For client to obtain RMI,
- remote object references may not be available at programming
- object names in text form are more convenient
** Java RMI Example
*** Polling
**** Architecture
***** Servant
Implement methods in remote interface
***** Server
- create remote objects
- register them in RMIregistry
***** Client
Lookup remote objects and access them
**** RMI registry
Binder for Java RMI
- map URL-style names \rightarrow remote object reference
- an instance of RMIregistry must run on each server
- two cases:
  - shared by all servers of different remote objects on same host (port 1099)
  - 1 registry per server, each different port
**** Implementation
***** How to compile
1. Compile remote interface
   - generate proxies
   - generate skeletons
2. Compile source codes
3. Start server
4. Start client
***** Example 1
- Remote interface
#+BEGIN_SRC java
import java.rmi.*;

public interface City extends Remote{
  int getPopulation(String x) throws RemoteException;
}
#+END_SRC

- Servant class
#+BEGIN_SRC java
import java.rmi.*;
import java.rmi.server.*;

public class CityImpl extends UnicastRemoteObject implements City{
  public CityImpl throws RemoteException{
    super();
  }
  public int getPopulation(String x) throws RemoteException{
  
  }
}
#+END_SRC
- Server class
#+BEGIN_SRC java
import java.rmi.*;
import java.rmi.server.*;

public class CityServer{
  public static void main(String args[]){
    try{
      CityImpl ci = new CityImpl();
      Naming.rebind("rmi://.../City",ci);
    }catch(Exception e){}
  }
}
#+END_SRC

- Client class
#+BEGIN_SRC java
import java.rmi.*;

public class CityClient{
  public static void main(String args[]){
    try{
      City c = (City) Naming.lookup("rmi://.../City");
      c.getPopulation("Toronto");
    }
    catch (RemoteException e){}
    catch (Exception e){}
  }
}
#+END_SRC

*** Callback
Server informs when updated.
How to implement?
- client creates callback remote object
#+BEGIN_SRC java
public interface Callback extends Remote{
  void cbMethod() throws RemoteException;
}
#+END_SRC
- server provides ~register(Callback cb)~ and ~deregister(Callback cb)~
#+BEGIN_SRC java
void register(Callback cbObject) throws RemoteException;
void deregister(Callback cbObject) throws RemoteException;
#+END_SRC
- when event occurs, server invokes method in ~Callback~
* Distributed File Systems
** Introduction
*** File System
- allow multiple clients to share access to files
- provides API to programmers

Elements:
- files
  - contain data and attributes
- directories
  - special types of files
  - mapping: text names \rightarrow internal file identifiers
  - serve as folders of files
  - form hierarchical structure
*** Distributed File System
**** Features
- support file accesses throughout an intranet
- new features
  - distributed naming and location
  - component to deal with client-server communication
**** Requirements
***** Transparency
- access
  - same interface for accesses to local & remote files
- location
  - uniform file name space
***** File replication & consistency maintenance
- to improve performance and enhance scalability
- enhance fault tolerance
- thus need to maintain consistency
***** Fault tolerance
****** Stateful
- remembers client's previous operations
- inter-dependent requests
- heavier server demand
- difficult setup and recovery on crashes
  - server must restore/notify after restart
  - server must detect client crashes and discard states
****** Stateless
- untracked independent self-contained requests
- easy setup and restore
- less burden
- heavier network demand
** Sun NFS
*** Overview
- access transparency
  - apps use same file operations for both local & remote
- no location tranparency
  - different clients can mount server dir to different local dirs
- stateless servers and idempotent operations
  - no need to recover state
  - server unaffected by client failure
- approximate one-copy update semantics
*** Architecture
- symmetric client-server relationship
  - each computer in network can act both as client/server
- OS independent design
- files are accessed by means of file IDs
  - file IDs is reference to file in the FS
  - file IDs are called file handles in NFS
    - FS ID
    - ID for file in FS
*** Client-Server Communication
**** NFS Server Interface
- operations used by NFS client in RPC
- not used directly by user programs
- stateles and idempotent operations
  - no open close and RW pointer
  - self contained
**** NFS Client
- supply interface suitable for use by conventional programs
  - emulates UNIX FS emantics
- transfer blocks of files to/from NFS servers
**** Mount Service
***** Overview
- server exports FS
  - mount service process runs
  - ~/etc/exports~:
    - names of local FS available for remote mounting
    - hosts permitted to mount
- client mounts FS
  - use modified UNIX mount to request mounting
    - remote host name
    - pathname in remote FS
    - local name
  - communicates with mount service using RPC protocol
    - request: remote dir pathname to server
    - reply: file handle of dir
  - maintains table of mounted FS
  - NFS doesn't enforce single network-wide file namespace
    - can assign different local names to remote directory
***** Pathname Translation
Suppose ~/~ of remote server is mounted. To access ~/bin/draw/readme~:
1. ~binfh = lookup(rootfh,"bin")~
2. ~drawfh = lookup(binfh,"draw")~
3. ~readmefh = lookup(drawfh,"readme")~
Efficiency can be improved by caching.
*** Client Caching
- caches results of RW etc
  - to reduce communication with server
  - block granularity
  - mainly in main memory
- comparison
  - local systems
    - one-copy update semantics
      - when file modified, all other clients see updates immediately
  - distributed systems
    - approximation to one-copy update semantics
- writes by client do not result in immediate updating of cached copies of same file in other clients
- potentially different versions of files across different clients
- to maintain consistency, clients should poll server
- cache is fresh when ~T-Tc < t~
- otherwise, need to poll validity

** AFS and Code File Systems
*** Overview
- access transparency
- high location transparency
- stateful
- relaxed consistency through session update semantics
- scalable
*** AFS Architecture
**** Features
- scalable
- two partitions of nodes
  - dedicated file servers
  - large number of clients
- similar to NFS
  - access to files via normal UNIX operations
  - venus clients and vice servers communicate using RPC
  - user programs use conventional UNIX pathnames to refer to files
  - venus translates pathnames into file IDs using step-by-step lookups from dirs in vice servers
  - files accesses by the obtained file IDs
**** Shared File Namespace
- shared file namespace
  - vice servers maintain global shared file namespace
  - venus clients follow
- clients have access to shared namespace by special local subdir e.g. /afs
  - client access file in /afs/ \rightarrow venus ensures appropriate part mounted
  - when mounting, venus ensures naming graph rooted at this subdir is always subgraph of complete shared namespace maintained by vice servers
  - shared files have same pathname
**** Volume
- easy replication, location, movement
  - basic unit of mounting and replication
  - smaller than UNIX FS
  - volume = partial subtree in shared namespace maintain by vice servers
**** Replication
- each file is contained in =1 logical volume
- volume may be replicated
- each logical volume is associated with a RVID
  - location and replication independent
- each physical volume has own VID
  - location dependent
- each shared file is identified by file ID
  - RVID + file handle
- consistency
  - read-one, write-all
**** Location
- Volume replication DB
  - RVID \rightarrow VIDs
- Volume location DB
  - VID \rightarrow server
So the file ID is transformed from RVID + file handle to Server + file handle after being passed to Volume replication DB and Volume location DB. Databases are fully replicated at each server.
*** Whole-File Serving and Caching
**** Typical UNIX Workloads
- Read more than write
- Entire, sequential file access
- Most files accessed only by one user
- Files are referenced in bursts (temporal locality)

**** Strategy
- Whole-file serving
  - entire files transmitted to clients
- Whole-file caching
  - transferred files are stored in cache
**** How it works
- Open \rightarrow entire file transmitted to client
- RWs performed on local copy
- Close \rightarrow client transmits updates to server, retain copy
**** Session update semantics
We relax the one-copy update semantics to reduce network traffic, performance degradation, be more scalable.

In session update semantics, all other clients are able to see a modified file only after the file is closed by the client that modified it.
**** Cache Consistency
***** Method
- foreach file, 
  - server keeps track which clients have copy of it
  - notify clients when file modified
***** Callback mechanism
- callback promise
  - valid: cache is fresh
  - cancelled: cache is out-of-date
- opening file
  - if not cached, fetch file and promise
  - if cached and valid, open local copy
  - if cached and cancelled, fetch copy and update promise
- closing file
  - if modified, send updates to server
- server
  - if file f is modified
    - foreach client holding valid cache of f
      - set client promise to cancelled
      - remove client from list of valid clients
***** Problem
If multiple clients write to a file concurrently, all updates are silently lost except those made by last client. Concurrency control must be implemented independently.
*** Coda File System
**** Overview
- designed to have high availability
  - client can continue operation when disconnected
- disconnected w.r.t. a volume: 
  - can't access any server having a copy of it
- ensure cache contains files to be accessed during disconnection
**** Hoarding
- ask user to specify useful files in a hoard DB
- compute priority from hoard DB + recent file access
- equlibrium
  - cached have higher priority than uncached
  - cache is full or all uncached have 0 priority
  - cached files are up-to-date
- priority may change overtime
- cached may be replaced
- periodically reorganize cache to maintain equilibrium
**** Client State
***** Hoarding
When connected, issue file requests to server(s) to perform work and attempts to keep cache filled with useful data
***** Emulation
When disconnected, requests are serviced using local copy
***** Reintegration
When reconnected, transfer updates to server
**** Conflicts
- during reintegration, conflicts may occur
- in such case, try conflict resolution
- if fail, manual intervention
- rare because UNIX \rightarrow only one user access a file
* P2P File Sharing Systems
** Introduction
- All processes play similar roles, interact cooperatively
- No distinction between clients and servers
- Every node provides some service that helps other nodes
- Resources at edge have intermittent connectivity
  - Added and removed from time to time
** Examples
*** File sharing
- Napster
- Gnutella
- KaZaA
- BitTorrent
*** Communication
- IM
- Skype VoIP
*** DHT
- Chord
** Unstructured P2P File Sharing
*** Overview
- simple to build
- simple to understand
- can't analyse performance
  - new node randomly chooses exinting nodes when joins
  - no coupling
  - random search
  - what is expected overhead of search?
- need many copies to make search overhead small
*** Napster
**** Overview
- centralized dir server
  - centralized search
- P2P distributed download
**** Steps
1. Connect to Napster server
2. Upload list of files to server
3. Give keywords to search with
4. Select best answers for downloading. User pings hosts that have the wanted file
5. User chooses server and downloads file
*** Gnutella
**** Overview
- Search servers that have file you want by flooding
  - BFS
- Reverse path forwarding for responses
**** How it works
- limit number of hops
  - user attaches TTL field to query message and init it with max allowable hop count
  - for nodes receiving query,
    - if TTL > 0, TTL-- and forward
    - if TTL = 0, stop
- expanded-ring TTL search
  - try TTL = 1.
  - if not found, try TTL = 2
  - ...
  - try TTL = max
**** Connection
1. bootstrap node to get IP addresses of existing Gnutella nodes
2. send join messages to some existing nodes
3. relations established
**** Issues
- flooding is not scalable
- may generate huge traffic
- possibly incomplete downloads
- ineffective search
*** KaZaA
**** Architecture
- hierarchical
- each peer is either supernode or node assigned to supernode
- if more bandwidth and more available, assigned as supernodes
- each supernode tracks contents and IP address of its children
**** How it works
***** Metadata
- when node connects to supernode, upload metadata
- foreach file
  - file name, size
  - ContentHash
  - file descriptors
***** Connection
- get list of potential supernodes
- find operational supernode
  - connects, obtain more up-to-date list
  - supernodes are close
  - new peer pings 5 supernodes and connects to one
- if down, node obtains updated list and chooses new supernode
***** Query
1. send keyword query to supernode
2. if < x matches found, supernode forward
3. if < x matches found, forward
4. result: ContentHash and list of nodes holding file
***** Download
- if available in multiple nodes, parallel downloading
- automatic recovery
  - client peer switch to new server peer when unavailable
  - ContentHash is used to search for new copy
** Structured DHT Systems
*** Overview
- tight coupling between nodes and file locations
- assign nodes to hold particular contents
- want to distribute responsibilities among nodes
- when a node requests that content, go to node that is supposed to have/know about it
- arrange neighbour relationship between nodes in a restrictive structure to facilitate query routing
*** DHT Services
**** The Hash
- hash function:
  - files \rightarrow [0,k]
- distribute [0,k] among all nodes
- node [l,r] is responsible for files that are mapped into l \leq x \leq r
  - direct
    - physically store files
  - indirect
    - stores locations of files
**** Routing
- query efficiently routed to node whose range covers file
- implemented in fully distributed manner
- each node maintains info of range spaces covered by some other nodes
- complexity must scale with system growth
- gracefully handle node joining/leaving system
  - bootstrap to connect
  - repartition range on joining/leaving nodes
  - reorganize/update routing info
*** Consistent Hashing
- Range [0,2^m-1]
- Let ..p n s .. be among the graph. Let f(i) be the assigned number to node i. Then node n is responsible for nodes [f(p)+1 mod 2^m-1,f(n) mod 2^m-1].
- Each file is held in a node with node_id \geq file_id
**** Scheme 1
- each node maintains ID and IP of successor
- low complexity of routing info
- high complexity of query routing
**** Scheme 2
- each node maintains ID and IP of all nodes
- low complexity of query routing
- high complexity of routing info
*** Chord
**** Method
- each node maintains ID and IP of nodes in a finger table
- ~table[n][i]~ = ID and IP of first node that succeeds n by at least 2^(i-1) on ID circle
  - each entry called a finger
- suppose want to query key k at node n, then lookup the table[n][log2(k-n)+1]
- O(lgN) routing information, O(lgN) query routing
- query can always be routed correctly as long as each node knows its correct successor node
**** Node Join
- if each node maintains info about its predecessor node, the node join is similar to doubly-linked list node join
  1. n uses bootstrapping process to find node
  2. n asks node to locate n's successor by key n
  3. n sets successor node
  4. n's predecessor is successor's predecessor
  5. n's predecessor's successor is n
  6. n's successor's predecessor is n
  7. build up finger table by querying
  8. let p be predecessor, s be successor
     - foreach i in [1..m],
       - foreach node x in [p-2^(i-1)+1,n-2^(i-1)],
         - table[x][i] = n
     log^2(N) complexity of node insertion
  9. n reports files that it is willing to share to nodes responsible for them
**** Node Leave
Similar to node join
* Name Services
** Names and Name Services
*** Names and Addresses
- Resources need to identify each other
- Names identify resources in a location-independent fashion
- Addresses identify resources in a location-dependent fashion
*** Name Service
- name \rightarrow address
- DNS: domain names \leftrightarrow IP address
- Java RMIregistry: remote object name \leftrightarrow remote object ref
- Name resolution: name \rightarrow address
  - may involve >1 translation
    - domain \rightarrow IP address \rightarrow MAC (by ARP)
  - pathname by filesystem
- Generally
  - address is just one of many attributes of resources
  - names \leftrightarrow attributes
*** Main Requirements
- scalability
  - arbitrary number of names
  - arbitrary number of administrative organizations
- reliability
  - high availability
  - no failure
- performance
  - shouldn't introduce much delay
*** Namespace
- collection of all valid names
  - requires syntactic definition
- can have hierarchical structure
  - each part resolved relative to a separate context
  - same name diff meanings in diff contexts
  - e.g. UNIX filesystem
  - more scalability
- can have flat structure
- e.g. DNS
  - hierarchical
  - domain name: \geq1 strings separated by dot
*** Name Resolution Process
- name server maintain name \rightarrow address mappings
  - binding: name \rightarrow resource
  - unbound: name doesn't correspond resource
- physically, resolution may involve more than 1 nameserver
  - may not be available at every nameserver; hierarchical namespace
  - iterative client-controlled navigation
  - non-recursive server-controlled navigation
  - recursive server-controlled navigation
*** Iterative Client-Controlled Navigation
- iteratively contacts a group of NS to resolve name
  - NS1,..,NS_i,NS_i+1,..
  - Potentially high communication cost if far away
- variation: multicast navigation
  - client multicasts name to group of NS
  - server holding name mapping responds
  - possibly faster response
  - possibly more network traffic
*** Nonrecursive Server-Controlled Navigation
- nameserver coordinates name resolution process on behalf of a client then passes back to client
- communicates by multicast/iteratively like a client
*** Recursive Server-Controlled Navigation
- recursively find until found
- caching results is more effective
- higher performance demand on each nameserver
  - may tie up server threads
  - may delay other requests
** Domain Name System
*** Overview
**** Scalability
- hierarchical namespace
- namespace is partitioned and managed by large group of nameservers
**** Reliability
- uses replication
**** Performance
- uses caching
*** Hierarchical Name Space
- DNS manage names of computers and domains
- domains associated with organizations
  - tree-like hierarchical structure refecting structure of organizations
- each domain manages own names
  - NTU gets name from whoever is in charge of edu.sg
  - Afterwards, it is free to assign hostnames within NTU w/o going back to edu.sg administration
*** Name Space Partitioning
- in original internet naming system, all hostnames and addresses were held in a single central master file downloaded by FTP to all computers
  - did not scale well
  - replaced by DNS
- DNS is decentralized
  - no single server is responsible for all names
  - entire name space is partitioned among largs group of name servers
  - each server holds authoritative mappings for names in one or more domains

*** DNS Name Servers
- information held
  - name \rightarrow IP addr for local domain
  - name & address of all name servers in local domain
  - name & address of name servers in its subdomains
  - name & address of root name servers
- each mapping contains: name+time-to-live+class+type+value
- allows multiple names to be mapped into same IP address
*** DNS Queries
**** Host name resolution
To resolve host names into IP addresses
**** Mail host location
To resolve domain names into IP addresses of mail hosts
*** Replication for Reliability
Authoritative mappings of each domain must be held by \geq2 name servers.
1. Primary server
   Reads mappings directly from local master file.
2. Secondary servers
   - Download mappings from primary server.
   - Periodically checks w/ primary server to see whether stored mappings are up-to-date

Root domain has about 12 secondary servers to share heavy workload.
*** DNS Name Resolution Process
- uses simple request-reply protocol over UDP
  - name server implements BIND
  - name servers use well known port number
  - DNS client is called resolver
- primarily uses combination of iterative client-controlled navigation and non-recursive server-controlled navigation
  - client contacts first server it knows of & progresses if unavailable
  - non-recursive server-controlled navigation is then used
*** Caching
- uses caching to improve performance
  - @ client
  - @ name server
- always consult caches before sending queries to other servers
- advantages
  - reduce response time
  - save network traffic
  - alleviate workload of high-level name servers
- introduces consistency problem
  - query answers returned from mappings are called non-authoritative answers
  - mappings and hence non-authoritative answers may be out-of-date
- timeout to address consistency problem
  - time-to-live values for cache expiry time
  - stale answers may still be provided before timeout
  - longer time-to-live \rightarrow weaker consistency
* Time and Global States
** Synchronizing Physical Clocks
*** Computer Clocks
- computers have their own physical clocks that deviate from one another
- *offset*: instantaneous difference between readings of two clocks
- *reference clock e.g. UTC*: ignals are broadcasted regularly from land-based radio stations and statellites around world
- *clock drift*: clocks count time at different rates
- *drift rate*: change in offset between a clock and a perfect reference clock per unit of time measured by reference clock
*** Cristian's Method
Suppose A wants to sync with B.
- A requests
- B replies T_B
- A records T_{round} = t_2 - t_1
- A sets clock to T_B + T_{round}/2

When A receives reply, time reading in B \in [T_B,T_B+T_{round}]. Taking the middle, A sets clock to T_B + T_{round}/2, so accuracy is +/- T_{round}/2.

To handle variability, make serveral requests to B and take min{T_{round}} for most accurate estimate.
*** Berkeley Algorithm
**** Definitions
***** Master
Coordinator computer
***** Slaves
Other computers whose clocks are to be sync'ed.
**** How it works
1. Master periodically polls slaves
2. Slaves reply with clocks
3. Master evaluates local times of slaves by T_{round}
4. Master averages values obtained
5. Master sends amount of adjustment to each slave

If master fails, elect another computer.
*** NTP
**** Comparison
- over internet, while former two are over intranets
- decentralized, while former two are centralized
**** Architecture
- a network of servers are structured hierarchically into synchronization subnet
  - primary servers (stratum 1) connected directly to time source
  - statum i are sync'ed with stratum i-1, \forall i \geq 2
  - Lowest level servers execute in users' workstations
**** Fault-tolerance
- servers can reconfigure themselves
  - if primary server UTC source fails, make it to be stratum 2 server.
  - if stratum 2 server's source of sync fails, sync with another primary server.
**** Message Exchange
- contains:
  - local times when previous NTP message between pair was sent&received.
  - local time when current NTP message was sent.
- Example
  - Suppose m is sent from A to B at T_{i-3} and arrives at T_{i-2}.
    Suppose m' is sent from B to A at T_{i-1} and arrives at T_{i}.
  - Total transmission time is (T_{i}-T_{i-3}) - (T_{i-1}-T_{i-2}).
    Transmission time of m' \in [0,(T_{i}-T_{i-3}) - (T_{i-1}-T_{i-2})].
    If it is 0, when A receives m', B's clock is T_{i-1}.
    If it is (T_{i}-T_{i-3}) - (T_{i-1}-T_{i-2}), when A receives m', B's clock is T_{i-1} + (T_{i}-T_{i-3}) - (T_{i-1}-T_{i-2}) = T_{i}-T_{i-3}+T_{i-2}
    In any case, when A receives m', B's clock reading \in [T_{i-1}, T_{i}-T_{i-3}+T_{i-2}]. Taking the middle point, A should set clock to 1/2 (T_{i-1} + T_{i}-T_{i-3}+T_{i-2}). Accuracy is +/- 1/2 (T_{i-1} - (T_{i}-T_{i-3}+T_{i-2})).
**** Misc
- applies data filtering to improve accuracy of synchronization over time
  - retain estimates of 8 most recent message exchanges and their accuracies
  - choose one with highest accuracy
- to choose peer, NTP server engages in message exchanges with several peers and applies peer-selection algo.
** Causal Ordering and Logical Clocks
*** System model
**** Distributed system
- N processes p_1,..,p_N
- each process on single processor
- no shared memory
- only way to communicate is through message passing through network
- reliable communication channels
- *asynchronous system*: no bounds on execution speed, transmission delays, clock drift rates
**** Process state
- values of all variables within the process
- values of any objects in local OS environment that process affects
- let s_i be state of process p_i
**** Process execution
- process takes a series of actions, either:
  - send message
  - receive message
  - internal action that transforms state
- *event*: occurence of a single action
*** Causal Ordering
- in asynchronous system, events can be ordered based on cause-and-effect
  - events within p_i can be placed in a unique total ordering
    - sequence of events within single process p_i is well defined
  - whenever message is sent between processes, event of sending message occurs before event of receiving message 
- denoted by \rightarrow
- is a *partial ordering*: there may exist pairs of events that remain unordered
- if !(e \rightarrow e') and !(e' \rightarrow e), then e || e'.
*** Lamport's Logical Clocks
**** Description
- *logical clock*: a monotonically increasing SW counter whose value bears no relationship with time
- p_i keeps logical clock L_i to timestamp events
**** How it works
1. \forall i, L_i = 0
2. Foreach event, increments L_i++ before timestamping it
3. When p_i sends m, t=L_i is piggybacked
4. When p_j receives (m,t), set L_j = max(L_j,t) before incrementing to timestamp
**** Theorem
e \rightarrow e' \Rightarrow L(e) < L(e')
*** Vector Clocks
**** Description
- V_i[1..N] = vector clock of process i
- Sets V_i[i]++ before timestamping each event at p_i
- When p_i sends m, t=V_i is piggybacked
- When p_j receives (m,t), set V_j[k] = max(V_j[k],t[k]) \forall k \in [1,N] before incrementing V_j[j] to timestamp
**** Rules
- V = V' \Leftrightarrow V[k] = V'[k] \forall k
- V \leq V' \Leftrightarrow V[k] \leq V'[k] \forall k
- V < V' \Leftrightarrow V \leq V' and V \neq V'
**** Theorem
- e \rightarrow e' \Leftrightarrow V(e) < V(e')
- Incomparable timestamps when concurrent:
  - e || e' \Leftrightarrow !(V(e) \leq V(e')) and !(V(e') \leq V(e))
** Global States
*** Motivation
Need to find out whether a property is true when DS executes
e.g. distributed deadlock detection
- each collection of processes waits for another
- cycle in grah of "wait-for" relationship
*** System Model
- *history of p_i*: all events within p_i and ordered by their occurences
  - h_i = <e_i^1,e_i^2,..>
- *prefix of history of p_i*: first k events in h_i
  - h_i^k = <e_i^1,e_i^2,..,e_i^k>
- *cut*: union of prefixes of histories
  - C = h_1^{c_1} \cup h_2^{c_2} \cup .. \cup h_N^{c_N}
- *global state*: set of states of all processes
  - let s_i^k be state of p_i after k^{th} event occurence
  - note that each global state <s_1^{c_1},s_2^{c_2},..,s_N^{c_N}> corresponds to a cut
*** Consistent Cut
- interested in cuts that are consistent
- *cut C is consistent*: \forall event e \in C, f \rightarrow e \Rightarrow f \in C
- otherwise, not consistent
- the corresponding global state of consistent cut is called a consistent global state
- in reality, system passes through consistent global states only
- gurantees 0 deadlock
*** Recording Snapshot
- steps
  1. p_1 records its local state
  2. p_1 sends m to p_2
  3. p_2 records its local state ASAP upon receival
- need to include state of communication channel
*** Chandy & Lamport Algorithm
**** Theorem
Chandy & Lamport Algorithm always selects a consistent cut
** Distributed Debugging
*** Description
- to establish what occurred during execution of DS
- to check whether they are correct
- but there is no global time!
- can use Chandy and Lamport to repeatedly record snapshots?
- *approach*: enumerate all consistent global states and check exhaustively
*** Lattice of Consistent Global States
- starting from initial state, advance one at a time
- each path from root to leaf represents an ordering of events that is consistent with causal ordering
- among possible paths, we can't tell which one actually executed since no global time
*** Observing Consistent Global States
- use additional monitor outside system to observe
- all processes in system send states to monitor
  - p_i sends its initial state to the monitor process initially
  - @e_i^{c_i}, p_i sends state to the monitor
- monitor assembles consistent global states
*** Global State Consistency Theorem
- *global state*: <s_1^{c_1},s_2^{c_2},..,s_N^{c_N}>
- V(e_i^{c_i}): vector clock timestamp of e_i^{c_i}
- <s_1^{c_1},s_2^{c_2},..,s_N^{c_N}> is consistent \Leftrightarrow \forall i,j, V(e_i^{c_i})[i] \geq V(e_j^{c_j})[i]
*** Debugging
How to check if constraint is broken?
1. construct lattice
2. foreach consistent global state, evaluate whether constraint satisfied
   - if all states satisfy, constraint can't be broken
   - if \exists state not satisfying, possible to be broken
   - if every path passes through \geq1 state not satisfying constraint, must be broken
* Coordination and Agreement
** Introduction
Shared values in DS is a problem.
- *mutual exclusion*: to coordinate access to shared resources
- *election*: to agree on a coordinator
- *consensus*: to agree on some value
** Distributed Mutex
*** Overview
Objective:
- prevents interference when a set of processes access shared resources
- extends critical section problem in OS

Distributed mutex:
- unlike standalone systems, in DS, neither shared variables nor facilities supplied by single local kernel can solve problem
- solution solely on *message passing*
*** System Model
- N processes p_1,..,p_N that don't share variables
- enter critical section to access shared resources
  - application-level protocol for CS
    - enter()
    - resourceAccess()
    - exit()
- asynchronous sytem
*** Requirements
- *safety*: \leq 1 process execute in CS at a time
- *liveness*: eventually succeeds
  - no deadlock
  - no starvation
*** Performance Metrics
- *bandwidth consumption*: # of messages sent each entry and exit 
- *client delay*: delay of each entry
- *throughput*: rate at which set of processes can access CS
  - synchronization delay: between exiting process and next entering process
  - focus on when process constantly wants to enter CS
*** Central Server Algorithm
**** Description
- server grants permission to enter CS
- to enter CS
  1. request to server and waits
  2. if no other process has token, reply with token
  3. otherwise, queue request
- to exit CS
  1. send token to server
  2. server selects oldest request from queue
**** Analysis
- *safety*: only 1 token
- *liveness*: FIFO ensures no deadlock/starvation
- not scalable due to centralized design
- *bandwidth consumption*
  - entering: 2 messages
  - exiting: 1 message
- *client delay*: 2 transmissions
- *synchronization delay*: 2 transmissions
*** Ring-Based Algorithms
Arrange processes in a logical ring. Circulate token around ring.
When received,
- if not wanted, forward token
- if wanted, retain token & enter CS

To exit CS, simply forward token to neighbour.

Analysis
- *safety*: only one token
- *liveness*: progress (no deadlock) and pass as soon as finish (no starvation)
- entry depends on location in ring, not necessarily on time order of request
- *bandwidth consumption*
  - if every process wants to enter CS, good
  - otherwise unbounded
- *client delay* \in [0,N] transmissions
- *sync delay* \in [1,N] transmissions
*** Ricart and Agrawala Algorithm
**** Description
- require total ordering of events
  - e.g. use Lamport logical clocks and break ties with PID
- process states \in {RELEASED,WANTED,HELD}
- entry
  1. state = WANTED
  2. send requests to all other processes
  3. wait for N-1 replies
  4. enter CS, state = HELD
- upon receival of request,
  1. if state = RELEASED or (state = WANTED and T_j < T_i)
     - reply
  2. otherwise, queue request
- exit
  1. state = RELEASED
  2. reply all queued requests
**** Analysis
- *safety*: since requests are totally ordered, safe
- *liveness*: FIFO queue \rightarrow no deadlock, no starvation
- *bandwidth consumption*: 2(N-1)
- *client delay*: 2 transmissions
- *sync delay*: 1 transmission
** Election
*** Introduction
- objective
  - choose unique process to play role, e.g. coordinator
  - if current crash/retires, need election to choose new one
- process initiates election if it starts particular run of election algo
  - 1 process doesn't initiate >1 election at a time
  - diff processes may call elections concurrently
  - more processes may fail during election
    - need to ensure non-crashed process is elected
- assume each process has PID
- WLOG, we require coordinator be chosen as process w/ largest PID
*** Performance Metrics
- *bandwidth consumption*: total #messages sent
- *T_{round}*: time between termination and initiation
*** Ring-Based Algorithm
**** Description
- assume asynchronous system and no failure
- process state \in {PARTICIPANT, NON_PARTICIPANT}
- initially, each process is NON_PARTICIPANT
- any process can initiate election:
  1. mark self as participant
  2. place PID in election message
  3. send to successor
- when process receives *election message*, compare message PID w/ own PID
  - if message PID > own PID, forward
  - if message PID < own PID and NON_PARTICIPANT, substitute & forward
  - if message PID < own PID and PARTICIPANT, stop
  - if message PID = own PID, be coordinator
- coordinator becomes NON_PARTICIPANT and sends *elected message* to neighbour, announce election, enclose identity
- when p_i receives election message
  - p_i becomes NON_PARTICIPANT
  - records coordinator PID
  - p_i forwards *elected message* to neighbour unless coordinator
**** Analysis
- correctness: involves all, unique even under concurrency
- *bandwidth consumption*
  - suppose only 1 initiating process
  - \leq N-1 election messages to get into largest
  - N more election messages to confirm
  - N elected messages for announcment of new coordinator
  - i.e. total is 3N-1 messages
- *T_{round}*
  - 3N-1 message transmissions since messages are sent sequentially

*** Bully Algorithm
**** Description
- suppose p_i knows processes p_j s.t. PID(p_j) > PID(p_i)
- three types of messages:
  - *ELECTION*: announce election
  - *ANSWER*: respond to election message
  - *COORDINATOR*: announce identity of coordinator
- if detect coordinator crash, begin election (may be concurrent)
- start election
  1. P sends election message to all p_j's and waits
  2. If within T no response, P wins and becomes coordinator
  3. If \exists p_j replying, P waits for coordinator message
- upon receival of election message from smaller PID
  1. send back answer message
  2. begin election if haven't
- if crashed process comes back up
  - if largest decide coordinator and announce
  - otherwise begin election
**** Analysis
- correctness
  - for any p_i, p_j, one with smaller PID discovers that the greater exists and defer to it
  - unique
- *bandwidth consumption*
  - suppose coordinator crash
  - best case: second largest detects crash \rightarrow 1 election message + (N-2) COORDINATOR messages
  - worst case: smallest detects crash \rightarrow O(N) election messages, each starts O(N) elections \rightarrow O(N^2)
- *T_{round}*: T + 1 transmission
** Consensus Problem
*** Introduction
Objective:
To agree on a value even in presence of failures after \geq 1 processes proposed value
- to transfer funds, processes must consistently agree to perform respective debit and credit
- in mutex, processes agree on who can enter CS
- in election, processes agree on who's the coordinator
- so consensus problem is a generalization
*** System Model
- N processes p_1,..,p_N communicating by message passing
- synchronous / asynchronous
- \leq f of N processes may fail
- every process p_i begins in UNDECIDED state, proposes v_i
- communicate to exchange values
- process p_i sets value of a decision variable d_i and enters DECIDED state

*** Requirements
**** Termination
Each correct process must set its decision variable d_i
**** Agreement
Decision values of of correct processes are same. d_1=..=d_N
**** Integrity
If correct processes all proposed same value, then all correct processes in DECIDED state has chosen that value
*** Synchronous
**** Assumptions
\leq f of N processes may crash
**** In absence of process crash
- each process p_i multicasts v_i to p_j \forall j
- each p_i waits until collected all N values and then evaluates a function to set decision variable
  - i.e. d_i = f(v_1,..,v_N)
**** When process crash
- faulty process may crash in the middle of multicast
  - may not send values to all p_j
- need to guarantee processes still receive same set of values
**** Solution
- proceeds f+1 rounds
  - each round, correct processes multicast values to p_j
  - in the end of (f+1)^{th} round, all correct processes that survive must have received same set of values
  - then each process evaluates a function to set decision variable
**** Theorem
Any algorithm to reach consensus despite \leq f crash failures requires \geq f+1 rounds of message exchanges

This lower bound also applies to case of byzantine failures.
*** Asynchronous = Impossible
In synchronous, message exchanges take place in rounds. If no message received within the round, assume process to be crashed.

In asynchronous, a crashed process is indistinguishable from a slow one.
* Replication and Consistency
** Introduction
- maintains copies of data at multiple copies
- improve performance and scalability
- increase reliability and fault-tolerance
- have to keep replicas consistent
** System Model
Each logical data object is implemented by a collection of replicas
*** Static vs Dynamic
- static: contains fixed set of replicas
- dynamic: replicas may be created/destroyed on the fly (not considered)
*** Transparency
- clients unaware of replicas
- clients see logical objects only
*** Data operations
- read
- write
- atomic operations @ replicas
*** Possible failures
- crash (process omission)
- byzantine (process)
- Network partition
*** Consistency model
- read returns value that shows result of last write
- but due to network elay, replicas are not necessarily consistent all time
- consistency models
  - define what is "correct" behavior
  - concern whether operations performed on collection of replicas produce results that meet correctness criteria
** Data-Centric Consistency Models
*** Overview
- target scenario: concurrent processes may simultaneously update data objects
- objective: to provide system-wide consistent view of data objects in face of concurrency
*** Strict concurrency
Absolute time ordering of all operations
*** Sequential concurrency
- all clients see all operations in same order
- operations may not be ordered in absolute time
*** Causal and FIFO
Different clients may see opeartions in different orders
**** Causal concurrency
All clients see causally-related operations in same order, but non-causally related operations may be viewed in different orders.
**** FIFO concurrency
All clients see writes from each other in the order they were issued, but other than that, can be seen in different orders.
** Data-Centric Consistency Protocols
*** Description
A consistency protocol describes an *implementation* of a specific consistency model.

We will look at fault-tolerant aspecs:
- does it handle faults?
- what type of faults?
- how many faults can it tolerate?
*** Primary-Based Protocols
**** Overview
- one primary replica, several backups
- passive replication
- primary replica coordinates writes
- if primary crashes, elect primary from backups
- examples:
  - remote-write: fixed primary replica
  - local-write: primary replica can be moved
**** Remote-write Protocol
***** Mechanism
- writes forwarded to primary
  1. primary performs update and forwards update to replicas
  2. each backup performs update
  3. primary send ack back to client
  - writes are blocking
- reads at any replica
***** A sequential consistency implementation
- primary replica orders writes in globally unique order
- all clients see all writes in same order, no matter which replica they use to perform reads
- due to blocking, clients always see effects of most recent writes
**** Local-write Protocol
- primary migrates between replicas that want to write
- writes can be non-blocking
  1. return ack to client after primary updates
  2. primary tells backups to update
- advantages:
  - multiple successive writes locally
  - enable disconnected mode
**** Fault tolerance
- can handle crashes
- to tolerate \leq f process crashes, must have f+1 replicas
- cannot deal with byzantine failures
  - only one replica executes operations
*** Replicated-Write Protocols
Writes at multiple replicas
**** Active replication
***** Description
- all replicas play *same role*
- writes at all replicas
  - operations sent to all via *totally ordered multicast*
  - all replicas start in same state and perform same operations in same order
  - writes are blocking
  - no action needed if a replica crash
- reads at any replicas
***** Analysis
- achieves sequential consistency
  - totally ordered multicast orders all writes
- fault tolerance
  - process omission:
    - client waits only for first response
  - byzantine:
    - to tolerate \leq f failures, 2f+1 replicas needed
    - client waits for f+1 identical responses
**** Gifford's quorum-based protocol
***** Introduction
****** Network Partitions
A network partition separates group of replicas into \geq 2 subgroups s.t.
- replicas of same subgroup can communicate with one anothr
- replicas of different subgroups can't communicate with one another

Partitions will eventually be repaired
- must ensure any operation executed during partition won't make replicas inconsistent when partition is repaired
****** Handling Network Partitions
- Optimistic approach
  - allow updates in all partitions
    - \rightarrow can lead to inconsistencies between partitions
  - resolve inconsistencies when partition is repaired
  - unsuitable for banking
- Pessimistic approach
  - allow updates in one partition only
    - \rightarrow prevents inconsistencies
  - update remaining replicas when partition is repaired
  - Gifford's quorum-based protocol
***** Design
- *quorum*: subgroup of replicas whose size gives it right to perform operations
- writes may be performed by one quorum only
  - use version numbers
  - writes only on latest replicas
- reads only on a read quorum (\geq R replicas)
- writes only on write quorum (\geq W replicas)
- W > 1/2 N
- R+W > N
***** Operations
****** Read
To perform read, client must assemble a read quorum
- contact a set of \geq R replicas
- set must contain at least one up-to-date replica
- read on any up-to-date replica
****** Write
Must assemble write quorum
- contact a set of \geq W replicas
- set must contain at least one up-to-date replica
- sync out-of-date replicas
- write is then applied to each replica in write quorum
** Client-Centric Consistency Models
*** Overview
- target scenario
  - lack of simultaneous updates (or easy-to-be-resolved updates)
  - most operations involve reading data
- eventual consistency
  - if no update takes place for a long time, all replicas will gradually become consistent
  - only requires that updates are guaranteed to propagaten (possibly lazy)
  - eventual consistency works fine as long as clients always access same replica
  - but problem if different replicas accessed
- guarantees *for a single client* concerning conistency
- no guarantees for concurrent accesses
*** Monotonic reads
- if client reads value of x @ r_i, any r_j for which j>i on x by that client will always return same value or a more recent value
- i.e. if seen x at t, will never see older version of x later
- e.g. distributed email database
*** Monotonic writes
write operation w_i by *a client* on x is completed before w_j j>i
*** Read your writes
Effect of a write operation by a client on x will always be seen by subsequent read operation on x by same client
*** Writes follow reads
- write by client on x following read by same client on x is guaranteed to take place on same/more recent value of x that was read
- writes on x are performed on copy of x that is up-to-date with value most recently read by that client
** Client-Centric Consistency Protocols
- each write is assigned a globally unique ID
  - can be assigned by origin replica of the write operation
- each client keeps track of 2 sets of write operations
  - read set: IDs of writes relevant to read operations by client
  - write set: IDs of writes performed by client

To implement monotonic-read consistency
- when client performs read operation at a replica, that replica is handed the client's *read set* to check whether all identified writes have taken place locally
- if not, replica contacts other replicas to ensure it is brought up to date
- alternatively, read operation is forwarded to a replica where the identified writes have already taken place
- after read, write operations that have taken place at selected replica and which are relevant for read operation are added to client's read set

Finally,
- write ID could include ID of replica performing the write
- replica could be required to log the write operation so that it can be replayed at another replica
- other client-centric consistency models can be implemented in similar ways
